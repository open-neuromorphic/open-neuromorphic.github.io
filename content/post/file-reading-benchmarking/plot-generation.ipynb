{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from expelliarmus import Wizard\n",
    "import aedat\n",
    "import event_stream\n",
    "import gc\n",
    "import hashlib\n",
    "import h5py\n",
    "import aestream\n",
    "import numpy as np\n",
    "import timeit\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "import loris\n",
    "import brotli\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fname = \"driving_sample\"\n",
    "fname = \"construction\"  # use this one if you want to include aedat and eventstream benchmarks\n",
    "\n",
    "# where to download and generate all the benchmark data\n",
    "folder = Path(\"data/file-benchmark\")\n",
    "folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# key is the name of the encoding, value is the file name ending\n",
    "extension_map = {\n",
    "    \"aedat\": \".aedat4\",\n",
    "    \"dat\": \".dat\",\n",
    "    \"evt2\": \"_evt2.raw\",\n",
    "    \"evt3\": \"_evt3.raw\",\n",
    "    \"hdf5\": \".hdf5\",\n",
    "    \"hdf5_lzf\": \"_lzf.hdf5\",\n",
    "    \"hdf5_gzip\": \"_gzip.hdf5\",\n",
    "    \"numpy\": \".npy\",\n",
    "    \"loris\": \".es\",\n",
    "    \"eventstream\": \".es\",\n",
    "    \"brotli\": \".bin.br\",\n",
    "    \"undr_numpy\": \".dvs\",\n",
    "    \"undr_brotli_11\": \".11.dvs.br\",\n",
    "    \"undr_brotli_6\": \".6.dvs.br\",\n",
    "    \"undr_brotli_1\": \".1.dvs.br\",\n",
    "}\n",
    "get_fpath = lambda encoding: f\"{folder}/{fname}{extension_map[encoding]}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the 'base' files\n",
    "These are the files with the original data, which will be loaded and then converted to all other formats under test. Currently you can choose between events from a Prophesee raw evt3 or an aedat4 sample file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_file_from_url(file_path, url):\n",
    "    print(f\"Downloading file to {file_path}... \")\n",
    "    r = requests.get(\n",
    "        url,\n",
    "        allow_redirects=True,\n",
    "    )\n",
    "    with open(f\"{file_path}.download\", \"wb\") as file:\n",
    "        file.write(r.content)\n",
    "    r.raise_for_status()\n",
    "    os.rename(f\"{file_path}.download\", file_path)\n",
    "    print(\"done!\")\n",
    "\n",
    "\n",
    "if fname == \"driving_sample\":\n",
    "    fpath = get_fpath(\"evt3\")\n",
    "    if not Path(fpath).is_file():\n",
    "        download_file_from_url(\n",
    "            fpath, \"https://dataset.prophesee.ai/index.php/s/nVcLLdWAnNzrmII/download\"\n",
    "        )\n",
    "    wizard = Wizard(encoding=\"evt3\")\n",
    "    events_ti8_xi2_yi2_pu1 = wizard.read(fpath)\n",
    "\n",
    "\n",
    "if fname == \"construction\":\n",
    "    aedat_fpath = get_fpath(\"aedat\")\n",
    "    if not Path(aedat_fpath).is_file():\n",
    "        download_file_from_url(\n",
    "            aedat_fpath,\n",
    "            \"https://cloudstor.aarnet.edu.au/plus/s/ORQ2oOz9NfwiHLZ/download?path=%2F&files=construction.aedat4\",\n",
    "        )\n",
    "    decoder = aedat.Decoder(aedat_fpath) # type: ignore\n",
    "    width = 0\n",
    "    height = 0\n",
    "    for stream in decoder.id_to_stream().values():\n",
    "        if stream[\"type\"] == \"events\":\n",
    "            width = stream[\"width\"]\n",
    "            height = stream[\"height\"]\n",
    "            break\n",
    "    assert width != 0\n",
    "    assert height != 0\n",
    "    events_tu8_xu2_yu2_onb = np.concatenate(\n",
    "        [packet[\"events\"] for packet in decoder if \"events\" in packet]\n",
    "    )\n",
    "    assert np.count_nonzero(np.diff(events_tu8_xu2_yu2_onb[\"t\"].astype(\"<i8\")) < 0) == 0\n",
    "    events_tu8_xu2_yu2_onb[\"t\"] -= events_tu8_xu2_yu2_onb[\"t\"][0]\n",
    "    events_ti8_xi2_yi2_pu1 = events_tu8_xu2_yu2_onb.astype(\n",
    "        np.dtype([(\"t\", \"<i8\"), (\"x\", \"<i2\"), (\"y\", \"<i2\"), (\"p\", \"u1\")], align=True)\n",
    "    )\n",
    "    events_tu8_xu2_yu2_pu1 = events_tu8_xu2_yu2_onb.astype(np.dtype([(\"t\", \"<u8\"), (\"x\", \"<u2\"), (\"y\", \"<u2\"), (\"p\", \"<u1\")]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate all comparison files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evt2 and dat\n",
    "raw_encodings = [\"dat\", \"evt2\", \"evt3\"]\n",
    "for encoding in raw_encodings:\n",
    "    fpath = get_fpath(encoding)\n",
    "    if not os.path.exists(fpath):\n",
    "        print(f\"Generating {fpath}.\")\n",
    "        wizard = Wizard(encoding=encoding)\n",
    "        wizard.save(fpath=fpath, arr=events_ti8_xi2_yi2_pu1)\n",
    "\n",
    "# variants of hdf5\n",
    "hdf5_encodings = [\"hdf5\", \"hdf5_lzf\", \"hdf5_gzip\"]\n",
    "for encoding in hdf5_encodings:\n",
    "    fpath = get_fpath(encoding)\n",
    "    if not os.path.exists(fpath):\n",
    "        with h5py.File(fpath, \"w\") as fp:\n",
    "            print(f\"Generating {fpath}.\")\n",
    "            dataset_dict = dict(\n",
    "                name=\"events\",\n",
    "                shape=events_ti8_xi2_yi2_pu1.shape,\n",
    "                dtype=events_ti8_xi2_yi2_pu1.dtype,\n",
    "                data=events_ti8_xi2_yi2_pu1,\n",
    "            )\n",
    "            if encoding == \"hdf5\":\n",
    "                fp.create_dataset(**dataset_dict)\n",
    "            elif encoding == \"hdf5_lzf\":\n",
    "                fp.create_dataset(**dataset_dict, compression=\"lzf\")\n",
    "            elif encoding == \"hdf5_gzip\":\n",
    "                fp.create_dataset(**dataset_dict, compression=\"gzip\")\n",
    "\n",
    "# Event Stream\n",
    "fpath = get_fpath(\"eventstream\")\n",
    "if not os.path.exists(fpath):\n",
    "    print(f\"Generating {fpath}.\")\n",
    "    with event_stream.Encoder(fpath, \"dvs\", width, height) as encoder:\n",
    "        encoder.write(events_tu8_xu2_yu2_onb)\n",
    "\n",
    "# numpy (pickle)\n",
    "fpath = get_fpath(\"numpy\")\n",
    "if not os.path.exists(fpath):\n",
    "    print(f\"Generating {fpath}.\")\n",
    "    np.save(fpath, events_ti8_xi2_yi2_pu1, allow_pickle=True)\n",
    "\n",
    "# numpy (UNDR)\n",
    "fpath = get_fpath(\"undr_numpy\")\n",
    "if not os.path.exists(fpath):\n",
    "    print(f\"Generating {fpath}.\")\n",
    "    events_tu8_xu2_yu2_pu1.tofile(fpath)\n",
    "\n",
    "# brotli (UNDR)\n",
    "brotli_qualities = [1, 6, 11]\n",
    "with open(get_fpath(\"undr_numpy\"), \"rb\") as uncompressed_file:\n",
    "    uncompressed_bytes = uncompressed_file.read()\n",
    "for quality in brotli_qualities:\n",
    "    fpath = get_fpath(f\"undr_brotli_{quality}\")\n",
    "    if not os.path.exists(fpath):\n",
    "        print(f\"Generating {fpath}.\")\n",
    "        compressed_bytes = brotli.compress(uncompressed_bytes, quality=quality)\n",
    "        with open(fpath, \"wb\") as compressed_file:\n",
    "            compressed_file.write(compressed_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aedat4\n",
    "# we do this in order to exclude IMU events that are part of the original file\n",
    "# see here for how to install dv https://dv-processing.inivation.com/rel_1.7/installation.html\n",
    "fpath = str(folder / \"construction_rewritten.aedat4\")\n",
    "if not os.path.exists(fpath):\n",
    "    print(f\"Generating {fpath}.\")\n",
    "    import dv_processing as dv\n",
    "    from tqdm.notebook import tqdm\n",
    "    store = dv.EventStore()\n",
    "\n",
    "    aedat_compatible_events = events_tu8_xu2_yu2_onb.astype(np.dtype([(\"t\", int), (\"x\", int), (\"y\", int), (\"p\", bool)]))\n",
    "    for event in tqdm(aedat_compatible_events):\n",
    "        store.push_back(timestamp=event[\"t\"], x=event[\"x\"], y=event[\"y\"], polarity=event[\"p\"])\n",
    "\n",
    "    resolution = (aedat_compatible_events['x'].max()+1, aedat_compatible_events['y'].max()+1)\n",
    "    config = dv.io.MonoCameraWriter.EventOnlyConfig(\"DVXplorer_sample\", resolution)\n",
    "    writer = dv.io.MonoCameraWriter(fpath, config)\n",
    "    writer.writeEvents(store)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPEAT = 5\n",
    "get_fsize_MiB = lambda fpath: round(fpath.stat().st_size / (1024 * 1024))\n",
    "\n",
    "def hash(events: np.ndarray, normalize_time: bool = False) -> str:\n",
    "    if normalize_time:\n",
    "        events[\"t\"] -= events[\"t\"][0]\n",
    "    result = hashlib.sha3_224()\n",
    "    result.update(events.astype(np.dtype([(\"t\", \"<u8\"), (\"x\", \"<u2\"), (\"y\", \"<u2\"), (\"on\", \"?\")])).tobytes())\n",
    "    return result.hexdigest()\n",
    "\n",
    "reference_hash = hash(events_tu8_xu2_yu2_onb)\n",
    "number_of_events = len(events_tu8_xu2_yu2_onb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete variables to minimize memory usage before the benchmarks\n",
    "\n",
    "del events_tu8_xu2_yu2_onb\n",
    "del events_ti8_xi2_yi2_pu1\n",
    "del events_tu8_xu2_yu2_pu1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evt2, evt3, dat\n",
    "expelliarmus_times = []\n",
    "expelliarmus_sizes = []\n",
    "for encoding in raw_encodings:\n",
    "    print(f\"Benchmarking expelliarmus ({encoding}).\")\n",
    "    gc.collect()\n",
    "    fpath = get_fpath(encoding)\n",
    "    def expelliarmus_read() -> np.ndarray:\n",
    "        wizard = Wizard(encoding)\n",
    "        wizard.set_file(fpath)\n",
    "        return wizard.read(fpath)\n",
    "    assert hash(expelliarmus_read()) == reference_hash\n",
    "    expelliarmus_times.append(timeit.timeit(expelliarmus_read, number=REPEAT) / REPEAT)\n",
    "    expelliarmus_sizes.append(get_fsize_MiB(Path(fpath)))\n",
    "\n",
    "# hdf5 variants\n",
    "hdf5_times = []\n",
    "hdf5_sizes = []\n",
    "for encoding in hdf5_encodings:\n",
    "    print(f\"Benchmarking HDF5 ({encoding}).\")\n",
    "    gc.collect()\n",
    "    fpath = get_fpath(encoding)\n",
    "    def hdf5_read() -> np.ndarray:\n",
    "        with h5py.File(fpath) as file:\n",
    "            return file[\"events\"][:] # type: ignore\n",
    "    assert hash(hdf5_read()) == reference_hash\n",
    "    hdf5_times.append(timeit.timeit(hdf5_read, number=REPEAT) / REPEAT)\n",
    "    hdf5_sizes.append(get_fsize_MiB(Path(fpath)))\n",
    "\n",
    "# brotli (UNDR)\n",
    "brotli_times = []\n",
    "brotli_sizes = []\n",
    "for quality in brotli_qualities:\n",
    "    print(f\"Benchmarking Brotli (Q={quality}).\")\n",
    "    gc.collect()\n",
    "    fpath = get_fpath(f\"undr_brotli_{quality}\")\n",
    "    def brotli_read() -> np.ndarray:\n",
    "        with open(fpath, \"rb\") as file:\n",
    "            return np.frombuffer(brotli.decompress(file.read()), dtype=np.dtype([(\"t\", \"<u8\"), (\"x\", \"<u2\"), (\"y\", \"<u2\"), (\"p\", \"<u1\")]))\n",
    "    assert hash(brotli_read()) == reference_hash\n",
    "    brotli_times.append(timeit.timeit(brotli_read, number=REPEAT) / REPEAT)\n",
    "    brotli_sizes.append(get_fsize_MiB(Path(fpath)))\n",
    "\n",
    "# numpy\n",
    "print(\"Benchmarking NumPy.\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"numpy\")\n",
    "def numpy_read() -> np.ndarray:\n",
    "    return np.load(fpath)\n",
    "numpy_time = timeit.timeit(numpy_read, number=REPEAT) / REPEAT\n",
    "numpy_size = get_fsize_MiB(Path(fpath))\n",
    "\n",
    "# numpy (UNDR)\n",
    "print(\"Benchmarking NumPy (UNDR).\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"undr_numpy\")\n",
    "def undr_numpy_read() -> np.ndarray:\n",
    "    return np.fromfile(fpath, dtype=np.dtype([(\"t\", \"<u8\"), (\"x\", \"<u2\"), (\"y\", \"<u2\"), (\"p\", \"<u1\")]))\n",
    "assert hash(undr_numpy_read()) == reference_hash\n",
    "undr_numpy_time = timeit.timeit(undr_numpy_read, number=REPEAT) / REPEAT\n",
    "undr_numpy_size = get_fsize_MiB(Path(fpath))\n",
    "\n",
    "# aedat4\n",
    "print(\"Benchmarking AEDAT.\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"aedat\")\n",
    "def aedat_read() -> np.ndarray:\n",
    "    decoder = aedat.Decoder(fpath) # type: ignore\n",
    "    return np.concatenate([packet[\"events\"] for packet in decoder if \"events\" in packet])\n",
    "assert hash(aedat_read(), normalize_time=True) == reference_hash\n",
    "aedat_time = timeit.timeit(aedat_read, number=REPEAT)/ REPEAT\n",
    "aedat_size = get_fsize_MiB(Path(fpath))\n",
    "\n",
    "# loris\n",
    "print(\"Benchmarking loris.\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"loris\")\n",
    "def loris_read() -> np.ndarray:\n",
    "    return loris.read_file(fpath)[\"events\"] # type: ignore\n",
    "assert hash(loris_read()) == reference_hash\n",
    "loris_time = timeit.timeit(loris_read, number=REPEAT) / REPEAT\n",
    "loris_size = get_fsize_MiB(Path(fpath))\n",
    "\n",
    "# eventstream\n",
    "print(\"Benchmarking eventstream.\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"eventstream\")\n",
    "def eventstream_read() -> np.ndarray:\n",
    "    with event_stream.Decoder(fpath) as decoder:\n",
    "        return np.concatenate([packet for packet in decoder])\n",
    "assert hash(eventstream_read()) == reference_hash\n",
    "eventstream_time = timeit.timeit(eventstream_read, number=REPEAT) / REPEAT\n",
    "eventstream_size = get_fsize_MiB(Path(fpath))\n",
    "\n",
    "# aestream\n",
    "print(\"Benchmarking AEStream.\")\n",
    "gc.collect()\n",
    "fpath = get_fpath(\"dat\")\n",
    "def aestream_read() -> np.ndarray:\n",
    "    return aestream.FileInput(fpath, (640, 480)).load()\n",
    "\n",
    "assert hash(aestream_read()) == reference_hash\n",
    "aestream_time = timeit.timeit(aestream_read, number=REPEAT) / REPEAT\n",
    "aestream_size = get_fsize_MiB(Path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dv_processing as dv\n",
    "\n",
    "print(\"Benchmarking DV.\")\n",
    "gc.collect()\n",
    "fpath = str(folder / \"construction_rewritten.aedat4\")\n",
    "\n",
    "def dv_read() -> np.ndarray:\n",
    "    reader = dv.io.MonoCameraRecording(fpath)\n",
    "    event_slices = []\n",
    "    while reader.isRunning():\n",
    "        slice = reader.getNextEventBatch()\n",
    "        if slice is None:\n",
    "            break\n",
    "        event_slices.append(slice.numpy())\n",
    "    return np.concatenate(event_slices)\n",
    "\n",
    "dv_time = timeit.timeit(dv_read, number=REPEAT) / REPEAT\n",
    "dv_size = get_fsize_MiB(Path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "\n",
    "results = (\n",
    "    list(\n",
    "        zip(\n",
    "            raw_encodings,\n",
    "            [\"expelliarmus\"] * len(raw_encodings),\n",
    "            expelliarmus_times,\n",
    "            expelliarmus_sizes,\n",
    "        )\n",
    "    )\n",
    "    + list(zip(hdf5_encodings, [\"h5py\"] * len(hdf5_encodings), hdf5_times, hdf5_sizes))\n",
    "    + list(\n",
    "        zip(\n",
    "            [f\"numpy/brotli (Q={quality})\" for quality in brotli_qualities],\n",
    "            [\"numpy/brotli\"] * len(brotli_qualities),\n",
    "            brotli_times,\n",
    "            brotli_sizes,\n",
    "        )\n",
    "    )\n",
    "    + [\n",
    "        (\"numpy (pickle)\", \"numpy\", numpy_time, numpy_size),\n",
    "        (\"numpy (UNDR)\", \"numpy\", undr_numpy_time, undr_numpy_size),\n",
    "        (\"aedat4\", \"aedat\", aedat_time, aedat_size),\n",
    "        (\"aedat4\", \"DV\", dv_time, dv_size),\n",
    "        (\"eventstream\", \"loris\", loris_time, loris_size),\n",
    "        (\"eventstream\", \"event_stream\", eventstream_time, eventstream_size),\n",
    "        (\"dat\", \"AEStream\", aestream_time, aestream_size),\n",
    "    ]\n",
    ")\n",
    "\n",
    "with open(\"results.json\", \"w\") as results_file:\n",
    "    json.dump(results, results_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Plot results\n",
    "\n",
    "import pandas\n",
    "import plotly.express\n",
    "\n",
    "with open(\"results.json\") as results_file:\n",
    "    results = json.load(results_file)\n",
    "dataframe = pandas.DataFrame(\n",
    "    {\n",
    "        \"Encoding\": [result[0] for result in results],\n",
    "        \"Framework\": [result[1] for result in results],\n",
    "        \"Read time [s]\": [result[2] for result in results],\n",
    "        \"File size [MiB]\": [result[3] for result in results],\n",
    "    }\n",
    ")\n",
    "\n",
    "title = f\"Reading the same {round(number_of_events / 1e6)} million events from different file formats.\"\n",
    "\n",
    "figure = plotly.express.scatter(\n",
    "    dataframe,\n",
    "    x=\"Read time [s]\",\n",
    "    y=\"File size [MiB]\",\n",
    "    color=\"Framework\",\n",
    "    symbol=\"Encoding\",\n",
    "    template=\"plotly_dark\",\n",
    "    title=title,\n",
    ")\n",
    "figure.update_traces(marker_size=13)\n",
    "figure.update_layout(height=600, width=900)\n",
    "figure.write_image(\"file_read_benchmark.png\")\n",
    "\n",
    "\n",
    "figure = plotly.express.scatter(\n",
    "    dataframe,\n",
    "    x=\"Read time [s]\",\n",
    "    y=\"File size [MiB]\",\n",
    "    color=\"Framework\",\n",
    "    symbol=\"Encoding\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "figure.update_traces(marker_size=13)\n",
    "figure.update_layout(height=400, width=1000, margin=dict(l=10,r=10,b=10,t=10),)\n",
    "figure.write_image(\"file_read_benchmark_white.png\")\n",
    "\n",
    "figure = plotly.express.scatter(\n",
    "    dataframe,\n",
    "    x=\"Read time [s]\",\n",
    "    y=\"File size [MiB]\",\n",
    "    color=\"Framework\",\n",
    "    symbol=\"Encoding\",\n",
    "    template=\"plotly_dark\",\n",
    "    title=title,\n",
    "    log_x=True,\n",
    "    log_y=True,\n",
    ")\n",
    "figure.update_traces(marker_size=13)\n",
    "figure.update_layout(height=600, width=900)\n",
    "figure.write_image(\"file_read_benchmark_log.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
