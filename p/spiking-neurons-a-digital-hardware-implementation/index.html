<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="In this article, we will try to model a Leaky Spiking Neuron (LIF) using digital hardware: registers, memories, adders and so on."><title>Spiking neurons: a digital hardware implementation</title><link rel=canonical href=https://open-neuromorphic.org/p/spiking-neurons-a-digital-hardware-implementation/><link rel=stylesheet href=/scss/style.min.8191399262444ab68b72a18c97392f5349be20a1615d77445be51e974c144cff.css><meta property="og:title" content="Spiking neurons: a digital hardware implementation"><meta property="og:description" content="In this article, we will try to model a Leaky Spiking Neuron (LIF) using digital hardware: registers, memories, adders and so on."><meta property="og:url" content="https://open-neuromorphic.org/p/spiking-neurons-a-digital-hardware-implementation/"><meta property="og:site_name" content="Open Neuromorphic"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="hardware"><meta property="article:tag" content="digital"><meta property="article:tag" content="spiking"><meta property="article:tag" content="snn"><meta property="article:tag" content="rtl"><meta property="article:tag" content="verilog"><meta property="article:tag" content="AI"><meta property="article:tag" content="machine learning"><meta property="article:published_time" content="2023-01-02T00:00:00+00:00"><meta property="article:modified_time" content="2023-01-02T00:00:00+00:00"><meta property="og:image" content="https://open-neuromorphic.org/p/spiking-neurons-a-digital-hardware-implementation/loihi.png"><meta name=twitter:title content="Spiking neurons: a digital hardware implementation"><meta name=twitter:description content="In this article, we will try to model a Leaky Spiking Neuron (LIF) using digital hardware: registers, memories, adders and so on."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://open-neuromorphic.org/p/spiking-neurons-a-digital-hardware-implementation/loihi.png"><link rel="shortcut icon" href=/img/ONM-logo.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/ONM-logo.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Open Neuromorphic</a></h1><h2 class=site-description>We are an organization of neuromorphic open source enthusiasts! We build tools for the community, host talks and discuss research. Click the Discord item below to join us and jump right in!</h2></div></header><ol class=social-menu><li><a href=https://discord.gg/C9bzWgNmqk target=_blank title=Discord rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-discord" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="9" cy="12" r="1"/><circle cx="15" cy="12" r="1"/><path d="M7.5 7.5c3.5-1 5.5-1 9 0"/><path d="M7 16.5c3.5 1 6.5 1 10 0"/><path d="M15.5 17c0 1 1.5 3 2 3 1.5.0 2.833-1.667 3.5-3 .667-1.667.5-5.833-1.5-11.5-1.457-1.015-3-1.34-4.5-1.5l-1 2.5"/><path d="M8.5 17c0 1-1.356 3-1.832 3-1.429.0-2.698-1.667-3.333-3-.635-1.667-.476-5.833 1.428-11.5C6.151 4.485 7.545 4.16 9 4l1 2.5"/></svg></a></li><li><a href=https://github.com/open-neuromorphic target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/groups/9267873 target=_blank title=LinkedIn rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="4" width="16" height="16" rx="2"/><line x1="8" y1="11" x2="8" y2="16"/><line x1="8" y1="8" x2="8" y2="8.01"/><line x1="12" y1="16" x2="12" y2="11"/><path d="M16 16v-3a2 2 0 00-4 0"/></svg></a></li><li><a href=https://www.youtube.com/@openneuromorphic target=_blank title=YouTube rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-youtube" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="3" y="5" width="18" height="14" rx="4"/><path d="M10 9l5 3-5 3z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/events/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-event" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="5" width="16" height="16" rx="2"/><line x1="16" y1="3" x2="16" y2="7"/><line x1="8" y1="3" x2="8" y2="7"/><line x1="4" y1="11" x2="20" y2="11"/><rect x="8" y="15" width="2" height="2"/></svg><span>Events</span></a></li><li><a href=/events-recordings/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-youtube" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="3" y="5" width="18" height="14" rx="4"/><path d="M10 9l5 3-5 3z"/></svg><span>Events recordings</span></a></li><li><a href=/resources/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-code" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><polyline points="7 8 3 12 7 16"/><polyline points="17 8 21 12 17 16"/><line x1="14" y1="4" x2="10" y2="20"/></svg><span>Resources</span></a></li><li><a href=/team/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-users" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="9" cy="7" r="4"/><path d="M3 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/><path d="M16 3.13a4 4 0 010 7.75"/><path d="M21 21v-2a4 4 0 00-3-3.85"/></svg><span>Team</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-open-source" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 3a9 9 0 013.618 17.243l-2.193-5.602a3 3 0 10-2.849.0l-2.193 5.603A9 9 0 0112 3z"/></svg><span>About</span></a></li><li><a href=/getting-involved/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-code" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><polyline points="7 8 3 12 7 16"/><polyline points="17 8 21 12 17 16"/><line x1="14" y1="4" x2="10" y2="20"/></svg><span>Getting involved</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#spiking-neurons>Spiking neurons</a></li><li><a href=#discretising-the-model>Discretising the model</a></li><li><a href=#storage-and-addressing-neurons-states>Storage and addressing neurons states</a></li><li><a href=#spikes-accumulation>Spikes accumulation</a></li><li><a href=#excitatory-and-inhibitory-neurons>Excitatory and inhibitory neurons</a></li><li><a href=#leakage>Leakage</a></li><li><a href=#spike-mechanism>Spike mechanism</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#acknowledgements>Acknowledgements</a></li><li><a href=#credits>Credits</a></li><li><a href=#authors>Authors</a></li><li><a href=#bibliography>Bibliography</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/spiking-neurons-a-digital-hardware-implementation/><img src=/p/spiking-neurons-a-digital-hardware-implementation/loihi_hubbeea76b711d6bc9f1399bce5b467dbf_1011331_800x0_resize_box_3.png srcset="/p/spiking-neurons-a-digital-hardware-implementation/loihi_hubbeea76b711d6bc9f1399bce5b467dbf_1011331_800x0_resize_box_3.png 800w, /p/spiking-neurons-a-digital-hardware-implementation/loihi_hubbeea76b711d6bc9f1399bce5b467dbf_1011331_1600x0_resize_box_3.png 1600w" width=800 height=778 loading=lazy alt="Featured image of post Spiking neurons: a digital hardware implementation"></a></div><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/p/spiking-neurons-a-digital-hardware-implementation/>Spiking neurons: a digital hardware implementation</a></h2><h3 class=article-subtitle>In this article, we will try to model a Leaky Spiking Neuron (LIF) using digital hardware: registers, memories, adders and so on.</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Jan 02, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>12 minute read</time></div></footer></div></header><section class=article-content><h2 id=spiking-neurons>Spiking neurons</h2><p>In this article, we will try to model a layer of Leaky Integrate and Fire (LIF) spiking neurons using digital hardware: registers, memories, adders and so on. To do so, we will consider a single output neuron connected to multiple input neurons from a previous layer.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/neurons-connected.png width=2061 height=744 srcset="/p/spiking-neurons-a-digital-hardware-implementation/neurons-connected_hufde41aca3783fa5bf23a53f4b1be2dc3_93109_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/neurons-connected_hufde41aca3783fa5bf23a53f4b1be2dc3_93109_1024x0_resize_box_3.png 1024w" loading=lazy alt="Multiple pre-synaptic neurons connected to a post-synaptic one." class=gallery-image data-flex-grow=277 data-flex-basis=664px></p><p>In a Spiking Neural Network (SNN), neurons communicate by means of <strong>spikes</strong>: these activation voltages are then converted to currents through the <strong>synapses</strong>, charging the <strong>membrane potential</strong> of the destination neuron. In the following, the destination neuron is denoted as <strong>post-synaptic</strong> neuron, with the index $i$, while the input neuron under consideration is denoted as <strong>pre-synaptic</strong> neuron, with the index $j$.</p><p>We denote the input spike train incoming from the pre-synaptic neuron with $\sigma_{j}(t)$:
$$ \sigma_{j}(t) = \sum_{k} \delta(t-t_{k}) $$
where $t_{k}$ are the spike timestamps of the spike train $\sigma_{j}(t)$.</p><p>The <strong>synapse</strong> connecting the pre-synaptic neuron with the post-synaptic neuron is denoted with $w_{ij}$. All the incoming spike trains are then <strong>integrated</strong> by the post-synaptic neuron membrane; the integration function can be modeled by a <strong>first-order low-pass filter</strong>, denoted with $\alpha_{i}(t)$:
$$ \alpha_{i}(t) = \frac{1}{\tau_{u_{i}}} e^{-\frac{t}{\tau_{u_{i}}}}$$
The spike train incoming from the pre-synaptic neuron, hence, is convolved with the membrane function; in real neurons, this corresponds to the <strong>input currents</strong> coming from the pre-synaptic neurons that <strong>charge</strong> the post-synaptic neuron membrane potential, $v_{i}(t)$. The sum of the currents in input to the post-synaptic neuron is denoted with $u_{i}(t)$ and modeled through the following equation:
$$ u_{i}(t) = \sum_{j \neq i}{w_{ij} \cdot (\alpha_{v} \ast \sigma_{j})(t)} $$
Each pre-synaptic neuron contributes with a current (spike train multiplied by the $w_{ij}$ synapse) and these sum up at the input of the post-synaptic neuron. Given the membrane potential of the destination neuron, denoted with $v_{i}(t)$, the differential equation describing its evolution through time is the following:
$$ \frac{\partial}{\partial t} v_{i}(t) = -\frac{1}{\tau_{v}} v_{i}(t) + u_{i}(t)$$
In addition to the input currents, we have the <strong>neuron leakage</strong>, $\frac{1}{\tau_{v}} v_{i}(t)$, modeled through a <strong>leakage coefficient</strong> $\frac{1}{\tau_{v}}$ that multiplies the membrane potential.</p><h2 id=discretising-the-model>Discretising the model</h2><p>Such a differential equation cannot be solved directly using discrete arithmetic, as it would be processed on digital hardware; hence, we need to <strong>discretise</strong> the equation. This discretisation leads to the following result:
$$ v_{i}[t] = \beta \cdot v_{i}[t-1] + (1 - \beta) \cdot u_{i}[t] - \theta \cdot S_{i}[t] $$
where $\beta$ is the <strong>decay coefficient</strong> associated to the leakage. We embed $(1-\beta)$ in the input current $u_{i}[t]$, by merging it with the synapse weights as a scaling factor; in this way, the input current $u_{i}[t]$ is <strong>normalised</strong> regardless of the decay constant $\tau_{v}$ value.</p><p>Notice that the <strong>membrane reset</strong> mechanism has been added: when a neuron <strong>spikes</strong>, its membrane potential goes back to the rest potential (usually equal to zero), and this is modeled by <strong>subtracting the threshold</strong> $\theta$ from $v_{i}(t)$ when an output spike occurs. The output spike is modeled through a function $S_{i}[t]$:
$$ S_{i}[t] = 1 ~\text{if}~ v_{i}[t] \gt \theta ~\text{else}~ 0 $$
This is equal to 1 at spike time (i.e. if at timestamp $t$ the membrane potential $v_{i}[t]$ is larger than the threshold $\theta$) and 0 elsewhere.</p><p>The input current is given by:
$$ u_{i}[t] = \sum_{j \neq i}{w_{ij} \cdot S_{j}[t]} $$<br>Notice that since $S_{i}[t]$ is either 0 or 1, the input current $u_{i}[t]$ is equal to the <strong>sum of the synapses weights</strong> of the pre-synaptic neurons that spike at timestamp $t$.</p><h2 id=storage-and-addressing-neurons-states>Storage and addressing neurons states</h2><p>Let us define the layer <strong>fan-in</strong>, i.e. how many pre-synaptic neurons are connected in input to each post-synaptic neuron in the layer; we denote this number with $N$. Then, we set the total number of neurons in our layer to $M$.</p><p>How do we describe a neuron in hardware? First of all, we need to list some basic information associated to each post-synaptic neuron:</p><ul><li>its <strong>membrane potential</strong> $v_{i}[t]$.</li><li>the <strong>weights associated with the synapses</strong>, $w_{ij}$; since each post-synaptic neuron is connected in input to $N$ neurons, these synapses can be grouped in an $N$-entries vector $W_{i}$.</li></ul><p>Since there are $M$ neurons in the layer, we need an $M$-entries vector, denoted with $V[t]$, to store the membrane potentials values evaluated at timestamp $t$; this vector is associated with a <strong>memory array</strong> in the hardware architecture.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/membrane-potentials.png width=1561 height=794 srcset="/p/spiking-neurons-a-digital-hardware-implementation/membrane-potentials_hu5bbfd51006a32eb425545f88180c0c97_62942_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/membrane-potentials_hu5bbfd51006a32eb425545f88180c0c97_62942_1024x0_resize_box_3.png 1024w" loading=lazy alt="The membrane potentials memory." class=gallery-image data-flex-grow=196 data-flex-basis=471px></p><p>An <strong>address</strong> is associated to each neuron, which can be thought as the $i$ index in the $V[t]$ vector; to obtain $v_{i}[t]$, the post-synaptic neuron address is used to index the membrane potentials memory $V[t]$.</p><p>We are able to store and retrieve a post-synaptic neuron membrane potential using a memory; now, we would like to <strong>charge it with the pre-synaptic neurons currents</strong> in order to emulate the behaviour of a neuron membrane; to do that, we need to get the corresponding input synapses $W_{i}$, <strong>multiply</strong> these by the spikes of the associated pre-synaptic neurons, sum them up and, then, accumulate these in the post-synaptic neuron membrane.</p><p>Let us start from a single input pre-synaptic neuron:
$$ u_{ij}[t] = w_{ij} \cdot S_{j}[t] $$
We know that $S_{j}[t]$ is either 1 or 0; hence, we have either $u_{ij}[t] = w_{ij}$ or $u_{ij}[t] = 0$; this means that the synapse weight is <strong>either added or not</strong> to the total current $u_{i}[t]$; hence, the weight $w_{ij}$ is read from memory <strong>only if the corresponding pre-synaptic neuron spikes!</strong> Given a layer of $M$ neurons, each of which is connected in input to $N$ synapses, we can think of grouping the $M \cdot N$ weights in a <strong>matrix</strong>, which can be associated with another memory array, denoted with $W$.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/synapses-weights.png width=2231 height=874 srcset="/p/spiking-neurons-a-digital-hardware-implementation/synapses-weights_hu67c5768f8825891815b89681ae240b1d_83094_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/synapses-weights_hu67c5768f8825891815b89681ae240b1d_83094_1024x0_resize_box_3.png 1024w" loading=lazy alt="The synapses weights memory." class=gallery-image data-flex-grow=255 data-flex-basis=612px></p><p>This memory is addressed with the pre-synaptic neuron and the post-synaptic neuron indices to retrieve the weight $w_{ij}$, which automatically corresponds to the $u_{ij}[t]$ current being accumulated in the post-synaptic neuron membrane when the pre-synaptic neuron spikes at timestamp $t$.</p><h2 id=spikes-accumulation>Spikes accumulation</h2><p>Let us implement neural functionalities using the data structures defined for a neuron (i.e. membrane potential and synapses), starting with the <strong>membrane potential charging</strong> of a post-synaptic neuron. When the pre-synaptic neuron spikes, its synapse weight $w_{ij}$ gets extracted from the synapse memory $W$ and multiplied by the spike; since the spike is a <strong>digital bit</strong> equal to 1, this is equivalent to <strong>using $w_{ij}$ itself as input current</strong> for the post-synaptic neuron; to add this current to $v_{i}[t]$, we need to use an arithmetic circuit called <strong>adder</strong>!</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/accumulation.png width=1612 height=1016 srcset="/p/spiking-neurons-a-digital-hardware-implementation/accumulation_hu0fc8f88676c742058e81209d5a8437fb_76462_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/accumulation_hu0fc8f88676c742058e81209d5a8437fb_76462_1024x0_resize_box_3.png 1024w" loading=lazy alt="The spikes accumulation circuit." class=gallery-image data-flex-grow=158 data-flex-basis=380px></p><p>The membrane potential $v_{i}[t]$ is read from the potentials memory $V[t]$ and added to the corresponding synapse current $w_{ij}$; the result is the membrane potential of the next time step, $v_{i}[t+1]$, that is stored in the <strong>register</strong> put on the adder output; this value is written back to the $V[t]$ memory in the next clock cycle. The register storing the adder output is denoted as <strong>membrane register</strong>.</p><p>To <strong>prevent multiple read-write cycles</strong> due to multiple spiking pre-synaptic neurons, one can think of adding a <strong>loop</strong> to the membrane register in order to <strong>accumulate all the currents</strong> of the pre-synaptic neurons that are spiking at timestep $t$ and writing the final value $v_{i}[t+1]$ back to memory <strong>only once</strong>. The corresponding circuit is shown in the following.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/accumulation-loop.png width=1799 height=1013 srcset="/p/spiking-neurons-a-digital-hardware-implementation/accumulation-loop_hu868ee57fbbd85ed207d780f73293926a_83987_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/accumulation-loop_hu868ee57fbbd85ed207d780f73293926a_83987_1024x0_resize_box_3.png 1024w" loading=lazy alt="Adding a loop register to accumulate multiple spikes before the write-back to memory." class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p>A <strong>multiplexer</strong> is placed on one side of the adder; in this way:</p><ul><li>the first weight $w_{i0}$ to be accumulated is added to the $v_{i}[t]$ read from memory and saved to the membrane register:
$$ v_{i}[t+1] = v_{i}[t] + w_{i0} $$</li><li>the successive weights are added to the membrane register content, so that all the currents are accumulated before writing $v_{i}[t+1]$ back to memory; using a non rigorous notation, this can be translated to the following equation:
$$ v_{i}[t+1] = v_{i}[t+1] + w_{ij},~ 0 \lt j \leq N $$</li></ul><h2 id=excitatory-and-inhibitory-neurons>Excitatory and inhibitory neurons</h2><p>Our post-synaptic neuron is able to accumulate spikes in its membrane; however, input spikes do not always result in membrane potential charging! In fact, a pre-synaptic neuron can be <strong>excitatory</strong> (i.e. it <strong>charges</strong> the post-synaptic neuron membrane) or <strong>inhibitory</strong> (i.e. it <strong>discharges</strong> the post-synaptic neuron membrane); in the digital circuit, this phenomenon corresponds to <strong>adding</strong> or <strong>subtracting</strong>, respectively, the synapse weight $w_{ij}$ to or from $v_{i}[t]$; this functionality can be added to the architecture by placing an adder capable of performing <strong>both additions and subtractions</strong>, choosing among these with a control signal generated by an <strong>FSM (Finite State Machine)</strong>, which is a sequential digital circuit that evolves through a series of states depending on its inputs and, consequently, generates controls signals for the rest of the circuit.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/inhibitory.png width=1612 height=1016 srcset="/p/spiking-neurons-a-digital-hardware-implementation/inhibitory_huca9ce09602971950892899452f33315f_93376_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/inhibitory_huca9ce09602971950892899452f33315f_93376_1024x0_resize_box_3.png 1024w" loading=lazy alt="Control circuit for choosing between excitatory and inhibitory stimula." class=gallery-image data-flex-grow=158 data-flex-basis=380px></p><p>This FSM, given the operation to be executed on the post-synaptic neuron, chooses if the adder has to add or subtract the synapse current.</p><p>However, is this design efficient in terms of resources employed? It has to be reminded that inhibitory and excitatory neurons are chosen at <strong>chip programming time</strong>; this means that <strong>the neuron type does not change during the chip operation</strong> (however, with the solution we are about to propose, it would not be a problem to change the neuron type on-the-fly); hence, we can <strong>embed this information</strong> in the neuron description by <strong>adding a bit to the synapses weights memory row</strong> that, depending on its value, denotes that neuron as excitatory or inhibitory.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/synapse-encoding.png width=400 height=146 srcset="/p/spiking-neurons-a-digital-hardware-implementation/synapse-encoding_hu15da4478632c6b64a6b83607aeaa5423_8608_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/synapse-encoding_hu15da4478632c6b64a6b83607aeaa5423_8608_1024x0_resize_box_3.png 1024w" loading=lazy alt="Synapses weight storage in memory." class=gallery-image data-flex-grow=273 data-flex-basis=657px></p><p>Suppose that, given a pre-synaptic neuron, all its $M$ output synapses are stored in a memory row of $n$ bits words, where $n$ is the number of bits to which the synapse weight is quantized. At the end of the memory row $j$, we add a bit denoted with $e_{j}$ that identifies the neuron type and that is read together with the weights from the same memory row: if the pre-synaptic neuron $j$ is <strong>excitatory</strong>, $e_{j}=1$ and the weight is <strong>added</strong>; if it is <strong>inhibitory</strong>, $e_{j}=0$ and the weight is <strong>subtracted</strong>; in this way, <strong>the $e_{j}$ field of the synapse can drive the adder directly</strong>.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/modified-adder.png width=1612 height=1016 srcset="/p/spiking-neurons-a-digital-hardware-implementation/modified-adder_huafc096ac915ea43ada89c8d177c22b8a_83317_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/modified-adder_huafc096ac915ea43ada89c8d177c22b8a_83317_1024x0_resize_box_3.png 1024w" loading=lazy alt="Using the neuron type bit to drive the adder." class=gallery-image data-flex-grow=158 data-flex-basis=380px></p><h2 id=leakage>Leakage</h2><p>Let us introduce the characteristic feature of the LIF neuron: the <strong>leakage</strong>! We shall choose a (constant) leakage factor $\beta$ and multiply it by $v_{i}[t]$ to obtain $v_{i}[t+1]$, which is <strong>lower</strong> than $v_{i}[t]$ since some current has leaked from the membrane, and we model this through $\beta$:
$$ v_{i}[t+1] = \beta \cdot v_{i}[t] $$
However, multiplication is an <strong>expensive</strong> operation in hardware; furthermore, the leakage factor is <strong>smaller than one</strong>, so we would need to perform a <strong>fixed-point multiplication</strong> or, even worse, a <strong>division</strong>! How can we solve this problem?</p><p>If we choose $\beta$ as a power of $\frac{1}{2}$, such as $2^{-n}$, the multiplication becomes <strong>equivalent to a $n$-positions right shift</strong>! A really <strong>hardware-friendly</strong> operation!</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/leak.png width=1612 height=1294 srcset="/p/spiking-neurons-a-digital-hardware-implementation/leak_hu6efb26cc282f1e71f25be47c1b1e2a9f_119349_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/leak_hu6efb26cc282f1e71f25be47c1b1e2a9f_119349_1024x0_resize_box_3.png 1024w" loading=lazy alt="Leakage circuit." class=gallery-image data-flex-grow=124 data-flex-basis=298px></p><p>In this circuit, an $n$-positions righ-shift block, denoted with the symbol <code>>></code>, is placed on one of the adder inputs to obtain $\beta \cdot v_{i}[t]$ from $v_{i}[t]$. A <strong>multiplexer</strong> is introduced to choose among the synapse weight $w_{ij}$ and the leakage contribution $\beta \cdot v_{i}[t]$ as input to the adder.</p><p>Notice that <strong>the leakage has to be always subtracted</strong> from the membrane potential; hence, we cannot use $e_{j}$ directly to control the adder but we must modify the circuit so that a subtraction is performed during a leakage operation, regardless of the value of $e_{j}$. A possible solution is to use a signal from the FSM and a <strong>logic AND gate</strong> to force the adder control signal to 0 during a leakage operation.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/subtract-leak.png width=1612 height=1294 srcset="/p/spiking-neurons-a-digital-hardware-implementation/subtract-leak_hud2de163dc45acbdca811dfc3794ca9d9_120661_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/subtract-leak_hud2de163dc45acbdca811dfc3794ca9d9_120661_1024x0_resize_box_3.png 1024w" loading=lazy alt="Simplified leakage circuit." class=gallery-image data-flex-grow=124 data-flex-basis=298px></p><p>Denoting with <code>adder_ctrl</code> the signal which controls the adder and with <code>leak_op_n</code> the one provided by the FSM, and stating that:</p><ul><li>for <code>adder_ctrl=1</code>, the adder performs an addition, otherwise a subtraction.</li><li><code>leak_op_n=0</code> when a leakage operation has to performed.</li></ul><p><code>adder_ctrl</code> can be obtained as the logic AND operation of <code>leak_op_n</code> and $e_{j}$ so that, when <code>leak_op_n=0</code>, <code>adder_ctrl=0</code> regardless of the value of $e_{j}$ and a subtraction is performed by the adder.</p><h2 id=spike-mechanism>Spike mechanism</h2><p>Our neuron needs to spike! If this is encoded as a single digital bit, given the spiking threshold $\theta$, we <strong>compare $v_{i}[t]$ to $\theta$</strong> and generate a logic 1 in output <strong>when the membrane potential is larger than the threshold</strong>. This can be implemented using a <strong>comparator</strong> circuit.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/spike.png width=1612 height=1635 srcset="/p/spiking-neurons-a-digital-hardware-implementation/spike_hu96016051d2ee2ee0e53c45c191e43005_138579_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/spike_hu96016051d2ee2ee0e53c45c191e43005_138579_1024x0_resize_box_3.png 1024w" loading=lazy alt="Spike circuit." class=gallery-image data-flex-grow=98 data-flex-basis=236px></p><p>The output of the comparator is used directly as <strong>spike bit</strong>.</p><p>The membrane has to be <strong>reset to a rest potential</strong> when the neuron spikes; hence, we need to <strong>subtract $\theta$ from $v_{i}[t]$ when the neuron fires</strong>. This can be done by driving the input multiplexer of the membrane register to <strong>provide $\theta$ in input to the adder</strong>, that performs a subtraction.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/reset.png width=1673 height=1635 srcset="/p/spiking-neurons-a-digital-hardware-implementation/reset_hu263ed7102fcea0fbf00e41d0963a3f26_147149_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/reset_hu263ed7102fcea0fbf00e41d0963a3f26_147149_1024x0_resize_box_3.png 1024w" loading=lazy alt="Membrane reset by threshold subtraction." class=gallery-image data-flex-grow=102 data-flex-basis=245px></p><p>This circuit can be simplified:</p><ul><li>by choosing $\theta = 2^m-1$, where $m$ is the <strong>bitwidth of the membrane register and the adder</strong>, having $v_{i}[t] \gt \theta$ is <strong>equivalent to having an overflow in the addition</strong>; hence, the comparison result is equal to the <strong>overflow flag</strong> of the adder, which can be <strong>provided directly in output as spike bit</strong>.</li><li>instead of subtracting $\theta$ from the membrane register, we can <strong>reset</strong> $v_{i}[t]$ to 0 when a spike occurs by forcing the membrane register content to 0 with a control signal; this is equivalent to using the oveflow flag of the adder as <strong>reset signal for the membrane register</strong>. This should not be done in an actual implementation: at least a <strong>register</strong> should be added on the reset signal of the membrane register to prevent glitches in the adder circuit from resetting it when it should not be.</li></ul><p>The resulting circuit is the following.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/smart-reset.png width=1756 height=1364 srcset="/p/spiking-neurons-a-digital-hardware-implementation/smart-reset_hud76884d98bd3d3ec19919eaa8e00ac8a_133275_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/smart-reset_hud76884d98bd3d3ec19919eaa8e00ac8a_133275_1024x0_resize_box_3.png 1024w" loading=lazy alt="Membrane reset by membrane potential zeroing." class=gallery-image data-flex-grow=128 data-flex-basis=308px></p><h2 id=conclusion>Conclusion</h2><p>Here we are, with a first prototype of our LIF layer digital circuit. In the next episode:</p><ul><li>we will make it actually work. Right now, this is a functional model, that needs some modifications to behave correctly as a spiking neurons layer.</li><li>we will implement it in Verilog.</li><li>we will simulate it using open source tools, such as <a class=link href=ihttps://www.veripool.org/verilator/>Verilator</a>.</li></ul><h2 id=acknowledgements>Acknowledgements</h2><p>I would like to thank <a class=link href=https://jasoneshraghian.com target=_blank rel=noopener>Jason Eshraghian</a>, <a class=link href=https://stevenabreu.com target=_blank rel=noopener>Steven Abreu</a> and <a class=link href=https://lenzgregor.com target=_blank rel=noopener>Gregor Lenz</a> for the valuable corrections and comments that made this article way better than the original draft!</p><h2 id=credits>Credits</h2><p>The cover image is the Loihi die, taken from <a class=link href=https://en.wikichip.org/wiki/intel/loihi target=_blank rel=noopener>WikiChip</a>.</p><h2 id=authors>Authors</h2><ul><li><a class=link href=https://fabrizio-ottati.dev target=_blank rel=noopener>Fabrizio Ottati</a> is a Ph.D. student in the HLS Laboratory of the Department of Electronics and Communications, Politecnico di Torino. His main interests are event-based cameras, digital hardware design and neuromorphic computing. He is one of the maintainers of two open source projects in the field of neuromorphic computing, <a class=link href=https://tonic.readthedocs.io target=_blank rel=noopener>Tonic</a> and <a class=link href=https://expelliarmus.readthedocs.io target=_blank rel=noopener>Expelliarmus</a>, and one of the founders of <a class=link href=https://open-neuromorphic.org target=_blank rel=noopener>Open Neuromorphic</a>.</li></ul><h2 id=bibliography>Bibliography</h2><ul><li><a class=link href=https://redwood.berkeley.edu/wp-content/uploads/2021/08/Davies2018.pdf target=_blank rel=noopener><em>Loihi: A Neuromorphic Manycore Processor with On-Chip Learning</em></a>, Mike Davies et al., 2018.</li><li><a class=link href=https://arxiv.org/abs/2109.12894 target=_blank rel=noopener><em>Training Spiking Neural Networks Using Lessons From Deep Learning</em></a>, Jason Eshraghian et al., 2022.</li><li><a class=link href=https://arxiv.org/abs/1804.07858 target=_blank rel=noopener><em>A 0.086-mm2 12.7-pJ/SOP 64k-Synapse 256-Neuron Online-Learning Digital Spiking Neuromorphic Processor in 28nm CMOS</em></a>, Charlotte Frenkel et al., 2019.</li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/hardware/>hardware</a>
<a href=/tags/digital/>digital</a>
<a href=/tags/spiking/>spiking</a>
<a href=/tags/snn/>snn</a>
<a href=/tags/rtl/>rtl</a>
<a href=/tags/verilog/>verilog</a>
<a href=/tags/ai/>AI</a>
<a href=/tags/machine-learning/>machine learning</a></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script></article><footer class=site-footer><section class=copyright>&copy;
2022 -
2023 Open Neuromorphic</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.16.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>